## Netty

常用组件:
- NioEventLoop: 事件线程池, 1.监听客户端连接. 2.处理输入输出流
- ByteBuff: byte 数组, 对应输入输出流
- Channel: socket 读写操作的一个抽象, 封装为 NioSocketChannel
- Pipeline: 逻辑链, DefaultChannelPipeline
- ChannelHandler: 逻辑链上的每一个处理

## Netty 服务端启动过程
1. 创建并初始化服务端 Channel
2. 注册 Selector
3. 端口绑定

    1.创建及初始化 Channel 流程(调用过程, 我使用层次的方式写, 表示方法内调用某方法):
    
            serverBootstrap.bind() 是创建入口(该方法位于AbstractBootStrap中)
                调用 doBind()
                    调用 initAndRegister()
                        调用 channelFactory.newChannel() // 内部使用反射机制创建 channel 对象, 创建的 channel类型就是 bootStrap.channel(NioServerSocketChannel.class) 中的参数类型
                            // NioServerSocketChannel 构造函数完成的内容
                                1. newSocket() // 创建 jdk 底层 channel
                                2. NioServerScocketChannelConfig // 配置 tcp 相关参数
                                3. 调用父类 AbstractNioChannel 的构造函数, 并设置 configureBlocking(false) 非阻塞模式, 调用 AbstractChannel() 创建id, unsafe, pipeline
                        调用 init() //初始化服务端 channel
                            set ChannelOptions, ChannelAttrs // 设置系统属性    
                            set ChildOptions, ChildAttrs // 当 accept 执行完, 都会对对应连接设置属性
                            config handler 配置服务端 pipeline
                            add ServerBootstrapAcceptor 往 pipeline 中添加服务连接器, 该连接器作用: 每当新连接接入, 则设置 options,attrs为上面配置的
                        调用 config().group().register(channel) 注册 channel (注册的入口)
                            this.eventLoop = eventLoop (绑定线程)
                            register0() (实际进行注册的方法)
                                doRegister() (调用 jdk 底层将 Channel 绑定到 Selector上 AbstractNioChannel 中)
                                invokeHandlerAddIfNeeded()
                                fireChannelRegistered() (传播事件)
                端口绑定
                    AbstractUnsafe.bind() (入口)
                        doBind()
                            java.Channel().bind() (jdk 底层的端口绑定)
                        pipeline.fireChannelActive() (传播事件)
                            HeadContext.readIfIsAutoRead() // 重新将注册到 select 上的事件设置为 accept 状态
        
    Channel 初始化流程: newChannel() -> init() -> register() -> doBind()

## NioEventLoop 解析
 
### 问题:
- netty 默认启多少线程, 何时启动
- netty 如何解决 jdk 空轮询
    - 解析:
    
        记录 select 空转的次数，定义一个阀值，这个阀值默认是 512 ，可以在应用层通过设置系统属性 io.netty.selectorAutoRebuildThreshold 传入，当空转的次数超过了这个阀值，重新构建新 Selector，将老 Selector 上注册的 Channel 转移到新建的 Selector 上，关闭老 Selector，用新的 Selector 代替老 Selector

- netty 如何保证异步串行无锁化
    - 解析:
    
        每个新连接, 都重新创建一个线程进行处理
        
### NioEventLoopGroup 三个模块(创建, 启动, 执行逻辑)

- 创建过程 (调用 MultithreadEventExecutorGroup 构造方法):
        
        注: MultithreadEventExecutorGroup 构造方法缩减为如下内容, 详细代码见源码.
        
        // 线程组, 默认 2*cpu
        new NioEventLoopGroup() 
        
            // 线程创建器, 就是 ThreadFactory (MultithreadEventExecutorGroup 类构造方法中), 每次执行任务使用 ThreadFactory 进行 new Tread() 操作
            executor = new ThreadPerTaskExecutor(newDefaultThreadFactory()); 
            
            // 构造 NioEventLoop (就是构建线程组)
            EventExecutor[] children;
            for(int i = 0; i < nThreads; i ++){
                // 创建线程任务执行器, newChild: 保存线程执行器
                children[i] = newChild(executor, args); 
            } 
            
            // 线程选择器, 给新连接绑定NioEventLoop 上, 也是说一个连接给一个线程处理
            chooserFactory.newChooser() 
- 启动(指执行任务, 启动线程):
        SingleThreadEventExecutor 类中的 execute 方法为启动入口
            startThread() -> doStartThread() -> run() // 如果当前线程之前没有创建过, 则重新创建, 然后执行
                调用 NioEventLoop 中的 run()
    
- 执行逻辑(接着启动中的 NioEventLoop 往后走):
      
        run() {
            for (;;) {
                select() // 检查是否有 io 事件, 此处的 select 与 jdk 原生 selector 效果一样
                    select 执行逻辑
                        Selector selector = this.selector;
                        int selectCnt = 0;
                        long currentTimeNanos = System.nanoTime();
                        long selectDeadLineNanos = currentTimeNanos + delayNanos(currentTimeNanos);
                        for (;;) {
                            deadline(内部设置 select 执行时间, 主要防止 select 空轮询) 以及任务穿插逻辑处理 // 到截止时间则 break, 或者有任务则执行任务
                            // 阻塞式 select
                            int selectedKeys = selector.select(timeoutMillis);
                            // 通过 selectCnt 以及任务执行时间, 判断是否 break
                            避免 jdk 空轮询 bug // 对 select 阻塞次数计算, 看是否出现空轮询 bug, 达到阈值, 则调用 selectRebuildSelector(selectCnt), 替换旧的 select
                        }
                processSelectedKeys() // 处理 io 事件
                    selected keySet 优化, 原本的 keySet 使用 hash 表结构, 将 hash 中内容放到数组中, 保证 O(1)
                    processSelectedKeysOptimized() // 处理每个 keySet
                runAllTasks() // 处理异步任务队列
            }
        }

## netty 新连接接入

### 问题:
- netty 如何检测新连接接入
- 新连接如何注册到NioEventLoop

### 新连接接入处理逻辑
    
- 检测新连接
        
        processSelectedKey(key, channel) 入口(NioEventLoop)
        方法源码如下:
            int readyOps = k.readyOps();
            if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {
                // 读取数据(在 AbstractNioMessageChannel 中有实现)
                unsafe.read();
                // read 中代码
                    do {
                        int localRead = doReadMessages(readBuf);
                        // doReadMessages 实现如下, 由下可知, 检测新连接时, 还是调用 Socket 底层 accept 进行判断
                        SocketChannel ch = SocketUtils.accept(javaChannel());
                    } while(allocHandle.continueReading());
            }
- 创建 NioSocketChannel (执行完上面的检测新连接逻辑后, 在 doReadMessages 方法中执行创建 NioSocketChannel 对象)
        
        // buf 是 list
        buf.add(new NioSocketChannel(this, ch));
            // 调用父类构造
            AbstractNioByteChannel(p, ch, op_read)
                // 设置连接为非阻塞模式
                configureBlocking(false) 并设置 op 为 read 状态
                创建连接 id, unsafe, pipeline
            new NioSocketChannelConfig()
    **channel 分类:**
    - AbstractNioByteChannel 客户端 channel, 处理读事件, 依赖于 NioByteUnsafe
    - AbstractNioMessageChannel 服务端 channel, 处理连接事件, 依赖于 NioMessageUnsafe
    - 注: XXXUnsafe 类设置事件处理方式

- 分配线程及注册 selector (接着检测新连接中的 doReadMessages 方法完后走)
        
        pipeline.fireChannelRead(readBuf.get(i)); 
            // fireChannelRead 内部调用 invokeChannelRead
            // 该方法将每一个连接, 从 head 传播到 tail, 这个传播链就是我们在 pipeline 中添加的 channelInboundHandler
            // 服务端 Channel 的 pipeline 构成: head -> serverBootstrapAcceptor -> tail
            static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) {
                    final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, "msg"), next);
                    EventExecutor executor = next.executor();
                    if (executor.inEventLoop()) {
                        // 调用的就是 ChannelInboundHandler 中的 channelRead(ctx, msg), 我们自己实现的那个 channelRead(ctx, msg)
                        next.invokeChannelRead(m);
                    } else {
                        executor.execute(()->next.invokeChannelRead(m));
                    }
                }
            
- 向 selector 注册读事件
    
    接着进入 NioEventLoop 中的 run 方法, 调用 select 方法注册读事件

## pipeline 解析:
### 问题: 
- netty 如何判断 channelHandler 类型
- 对于 channelHandler 的添加遵循什么样的原则
- 用户手动触发事件传播, 不同的触发方式有什么区别
    
### DefaultChannelPipeline 类解析
- pipeline 初始化(DefaultChannelPipeline 类)
        
        pipeline 在创建 channel 的时候被创建
            在 AbstractChannel 的构造方法中, pipeline = newChannelPipeline(), 对应 DefaultChannelPipeline 类的构造函数
            构造方法中
        pipeline 节点数据结构: AbstractChannelHandlerContext head, tail
        pipeline 中哨兵 head, tail
            HeadContext: 将事件往下传播, 属于 outbound
            TailContext: 收尾工作, 属于 inbound

- 添加删除 channelHandler (add 方法及 remove 方法)
        
        判断是否重复添加 
            在 addLast 方法中, checkMultiplicity(handler) // 检查当前 handler 是否可共享的, 当前 handler 是否已经添加过了
        创建节点并添加至链表
            newCtx = newContext(group， filterName(name, handler), handler);
            addLast0(newCtx);
        回调添加完成事件
            使用 executor 调用 callHandlerAdded0(newCtx); 添加节点, 并设置当前节点已经添加完毕

        删除 ChannelHandler
            remove0 // 找到节点: 在 context(ChannelHandler handler) 中顺序遍历, 找到对应节点从链表中删除
            callHandlerRemoved0(ctx) // 回调删除 Handler 事件
- 事件和异常的传播
        
        inbound 事件传播
            inbound 事件及 channelInboundHandler
            ChannelRead 事件的传播
                内部使用 fireChannelRead(msg) 对事件进行传播
                传播顺序按照 channelInboundHandler 添加顺序一致
            SimpleInBoundHandler 处理器
                常规的 InBound 处理完数据需要将数据往后传播, 最终由 tail 节点进行回收操作, 但是中间没有进行往后传递, ByteBuf 容易出现内存泄漏的问题, SimpleInBoundHandler channelRead 在 finally 中会自动进行释放操作
        outbound 事件传播
            传播顺序与 pipeline.addLast(channelOutboundHandler) 添加顺序相反, 事件传播总是从 tail 位置开始, 往前调用
            // AbstractChannelHandlerContext 中 fireChannelRead 方法可见
            invokeChannelRead(findContextInbound(MASK_CHANNEL_READ), msg);
                // findContextInbound 代码如下, 从链表往前遍历, 执行对应事件方法
                    ctx = ctx.prev;
                } while ((ctx.executionMask & mask) == 0);
            当事件传播到 head 的时候, 

        inbound 更多是 register, active, read, 事件被动的触发
        outboud 则是 read, write, flush, 事件主动触发
        
        异常触发链(建议自行编写程序 debug 调试观察该调用过程, 此处只说结论):
            当 head 到 tail 中某个节点(无论是 ChannelInbound 或者 ChannelOutbound)抛出异常, 那么
            异常沿着 next 往后面的节点传播, 不再像之前的 ChannelInbound, ChannelOutbound 的传播顺序,
            当异常到达 tailContext 时, 就不再进行传播, 打印日志表示没有节点能处理该异常, 并不再往前处理
            

    
